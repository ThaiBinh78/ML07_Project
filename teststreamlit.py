# app_motor_price.py
import streamlit as st
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import traceback

# ----------------------
# CONFIG (use uploaded files in /mnt/data if present)
# ----------------------
# If you uploaded models to the container, these are typical paths:
MODEL_PATH = Path("/mnt/data/rf_pipeline.pkl")
ISO_PATH = Path("/mnt/data/isolation_forest.pkl")
SAMPLE_PATH = Path("/mnt/data/sample_data.csv")
FI_CSV = Path("/mnt/data/feature_importances.csv")

# fallback to repo-local files if /mnt/data doesn't exist
if not MODEL_PATH.exists():
    MODEL_PATH = Path("rf_pipeline.pkl")
if not ISO_PATH.exists():
    ISO_PATH = Path("isolation_forest.pkl")
if not SAMPLE_PATH.exists():
    SAMPLE_PATH = Path("sample_data.csv")
if not FI_CSV.exists():
    FI_CSV = Path("feature_importances.csv")

PENDING_PATH = Path("pending_listings.csv")
LOG_PATH = Path("prediction_logs.csv")

CURRENT_YEAR = datetime.now().year

st.set_page_config(page_title="D·ª± ƒëo√°n gi√° - Xe m√°y c≈©", layout="wide")

# ----------------------
# Helpers
# ----------------------
@st.cache_resource
def load_models_and_sample(rf_path: Path, iso_path: Path, sample_path: Path):
    """
    Load model, iso, sample. Normalize sample column names so downstream code is stable.
    """
    model = joblib.load(str(rf_path))
    iso = joblib.load(str(iso_path))
    sample = pd.read_csv(str(sample_path))
    sample = sample.rename(columns=lambda x: x.strip())
    # unify price column to 'Gia_trieu' numeric (tri·ªáu)
    if 'Gia_trieu' not in sample.columns and 'Gia_trieu' not in sample.columns and 'Gia_trieu' not in sample.columns:
        if 'Gia_trieu' in sample.columns:
            sample['Gia_trieu'] = pd.to_numeric(sample['Gia_trieu'], errors='coerce')
    if 'Gia_trieu' not in sample.columns and 'Gi√°' in sample.columns:
        sample['Gia_trieu'] = pd.to_numeric(sample['Gi√°'], errors='coerce')
    # coerce min/max if present
    for col in ["Kho·∫£ng gi√° min", "Kho·∫£ng gi√° max", "Gi√°", "Gia_trieu"]:
        if col in sample.columns:
            sample[col] = pd.to_numeric(sample[col], errors='coerce')
    return model, iso, sample

def ensure_cols_for_upload(df: pd.DataFrame):
    required = [
        "Th∆∞∆°ng_hi·ªáu","D√≤ng_xe","Lo·∫°i_xe","Dung_t√≠ch_xe",
        "NƒÉm_ƒëƒÉng_k√Ω","S·ªë_Km_ƒë√£_ƒëi","Gi√°","Kho·∫£ng_gi√°_min","Kho·∫£ng_gi√°_max",
        "Ti√™u_ƒë·ªÅ","M√¥_t·∫£_chi_ti·∫øt","ƒê·ªãa_ch·ªâ","Xu·∫•t_x·ª©"
    ]
    missing = [c for c in required if c not in df.columns]
    return missing

def add_pending(entry: dict):
    if PENDING_PATH.exists():
        df = pd.read_csv(PENDING_PATH)
    else:
        df = pd.DataFrame()
    entry_id = int(datetime.utcnow().timestamp() * 1000)
    entry["id"] = entry_id
    df = pd.concat([pd.DataFrame([entry]), df], ignore_index=True, sort=False)
    df.to_csv(PENDING_PATH, index=False)
    return entry_id

def log_prediction(record: dict):
    if LOG_PATH.exists():
        logs = pd.read_csv(LOG_PATH)
    else:
        logs = pd.DataFrame()
    logs = pd.concat([pd.DataFrame([record]), logs], ignore_index=True, sort=False)
    logs.to_csv(LOG_PATH, index=False)

def human_currency_trieu(x):
    try:
        v = float(x)
        return f"{v:,.2f} Tri·ªáu"
    except:
        return x

def compute_anomaly_score_v2(sample_df, brand, actual_price, pred, iso, X_trans_for_iso):
    """
    New, simpler and explainable anomaly score (0-100):
      - Price Gap % component (60%): gap_pct = |actual - pred| / pred * 100
          - If no actual_price (NaN): we don't compute gap component (weight redistributed)
      - P10/P90 component (20%): if actual outside [P10,P90] of brand
      - ISO component (20%): normalized iso_score_raw
    Returns final_score and details (friendly).
    """
    # ensure pred numeric
    pred_val = float(pred)
    # price gap
    has_price = (actual_price is not None) and (not pd.isna(actual_price))
    gap_pct = None
    score_gap = 0.0
    weight_gap = 0.6
    weight_p10p90 = 0.2
    weight_iso = 0.2

    if has_price and pred_val != 0:
        gap_pct = abs(float(actual_price) - pred_val) / abs(pred_val) * 100.0
        # map gap_pct to 0-100 (cap at 200% for safety)
        score_gap = min(100.0, gap_pct)  # 100 means >=100% gap
    else:
        # If no price provided, remove gap component and renormalize weights
        weight_gap = 0.0
        total_remain = weight_p10p90 + weight_iso
        if total_remain > 0:
            weight_p10p90 = weight_p10p90 / total_remain
            weight_iso = weight_iso / total_remain

    # brand distribution
    sample_brand = pd.DataFrame()
    if 'Th∆∞∆°ng hi·ªáu' in sample_df.columns:
        sample_brand = sample_df[sample_df['Th∆∞∆°ng hi·ªáu'] == brand].copy()
    if 'Th∆∞∆°ng_hi·ªáu' in sample_df.columns and sample_brand.empty:
        sample_brand = sample_df[sample_df['Th∆∞∆°ng_hi·ªáu'] == brand].copy()

    # P10/P90 flag
    p10 = np.nan
    p90 = np.nan
    score_p10p90 = 0.0
    if len(sample_brand) > 0 and 'Gia_trieu' in sample_brand.columns:
        p10 = sample_brand['Gia_trieu'].quantile(0.10)
        p90 = sample_brand['Gia_trieu'].quantile(0.90)
        if has_price and (not pd.isna(p10)) and (not pd.isna(p90)):
            if actual_price < p10:
                # lower tail -> map distance to score (smaller actual -> higher score)
                frac = (p10 - actual_price) / max(1.0, p10)
                score_p10p90 = min(100.0, frac * 100.0)
            elif actual_price > p90:
                frac = (actual_price - p90) / max(1.0, p90)
                score_p10p90 = min(100.0, frac * 100.0)
    else:
        # insufficient brand data -> fallback 0
        score_p10p90 = 0.0

    # ISO: compute raw score (higher -> more anomalous); normalize to 0..100 by heuristic
    iso_score_raw = 0.0
    iso_flag = 0
    try:
        iso_vec = X_trans_for_iso
        if hasattr(iso_vec, "toarray"):
            iso_vec = iso_vec.toarray()
        iso_vec = np.asarray(iso_vec)
        if iso_vec.ndim == 1:
            iso_vec = iso_vec.reshape(1, -1)
        iso_score_raw = - iso.decision_function(iso_vec)[0]
        iso_flag = int(iso.predict(iso_vec)[0] == -1)
    except Exception:
        iso_score_raw = 0.0
        iso_flag = 0
    # normalize iso_score_raw: assume typical raw range ~ [0..1], scale *100 and cap
    score_iso = min(100.0, max(0.0, iso_score_raw * 100.0))

    # final weighted score
    final_score = weight_gap * (score_gap) + weight_p10p90 * (score_p10p90) + weight_iso * (score_iso)

    # Compose friendly explanation
    reasons = []
    if has_price and gap_pct is not None:
        if gap_pct >= 50:
            reasons.append(f"- Gi√° th·ª±c l·ªách so v·ªõi d·ª± ƒëo√°n {gap_pct:.0f}% (l·ªõn).")
        elif gap_pct >= 20:
            reasons.append(f"- Gi√° th·ª±c l·ªách so v·ªõi d·ª± ƒëo√°n {gap_pct:.0f}% (kh√° ƒë√°ng ch√∫ √Ω).")
    if score_p10p90 > 0:
        if actual_price < p10:
            reasons.append(f"- Gi√° th·∫•p h∆°n P10 c·ªßa th∆∞∆°ng hi·ªáu (P10={p10:.2f} Tri·ªáu).")
        else:
            reasons.append(f"- Gi√° cao h∆°n P90 c·ªßa th∆∞∆°ng hi·ªáu (P90={p90:.2f} Tri·ªáu).")
    if iso_flag:
        reasons.append(f"- M·∫´u tin c√≥ ƒë·∫∑c ƒëi·ªÉm l·∫° so v·ªõi d·ªØ li·ªáu hu·∫•n luy·ªán (IsolationForest).")
    if not reasons:
        reasons.append("- Kh√¥ng ph√°t hi·ªán ƒëi·ªÉm b·∫•t th∆∞·ªùng r√µ r·ªát d·ª±a tr√™n 3 ti√™u ch√≠.")

    details = {
        "has_price": bool(has_price),
        "gap_pct": (gap_pct if gap_pct is not None else np.nan),
        "score_gap": score_gap,
        "score_p10p90": score_p10p90,
        "score_iso": score_iso,
        "iso_flag": int(iso_flag),
        "iso_score_raw": float(iso_score_raw),
        "final_score": float(final_score),
        "explanations": reasons
    }

    return float(final_score), details

# ----------------------
# Load models & sample (safe)
# ----------------------
try:
    missing = [p for p in [MODEL_PATH, ISO_PATH, SAMPLE_PATH] if not Path(p).exists()]
    if missing:
        raise FileNotFoundError(f"Missing files: {[str(x) for x in missing]}. Make sure these files are in the same folder as this app or in /mnt/data.")
    model, iso, sample_df = load_models_and_sample(MODEL_PATH, ISO_PATH, SAMPLE_PATH)
except Exception as e:
    st.error("Kh√¥ng th·ªÉ load model/sample. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n & file c√≥ trong repo hay kh√¥ng.")
    st.write(str(e))
    st.write(traceback.format_exc())
    st.stop()

# ----------------------
# Sidebar & Pages
# ----------------------
st.sidebar.title("Menu")
if Path("xe_may_cu.jpg").exists():
    st.sidebar.image("xe_may_cu.jpg", use_column_width=True)
page = st.sidebar.radio("Ch·ªçn m·ª•c", ["Business Problem", "Prediction", "Anomaly Check", "Admin Dashboard", "Logs", "Evaluation & Report", "Team Info"])

# Business Problem
def render_business_problem():
    st.title("Business Problem")
    st.markdown("""
- **M·ª•c ti√™u:** D·ª± ƒëo√°n gi√° b√°n h·ª£p l√Ω cho xe m√°y c≈© v√† ph√°t hi·ªán tin ƒëƒÉng c√≥ gi√° b·∫•t th∆∞·ªùng.
- **Input:** Th∆∞∆°ng hi·ªáu, D√≤ng xe, NƒÉm ƒëƒÉng k√Ω, S·ªë Km, Lo·∫°i xe, Dung t√≠ch, Xu·∫•t x·ª©, (Gi√° th·ª±c - t√πy ch·ªçn).
- **Output:** Gi√° d·ª± ƒëo√°n (Tri·ªáu VNƒê), **Price Risk Score (0-100)**, K·∫øt lu·∫≠n (Gi√° th·∫•p b·∫•t th∆∞·ªùng / Gi√° cao b·∫•t th∆∞·ªùng / B√¨nh th∆∞·ªùng).
- **Ph∆∞∆°ng ph√°p:** RandomForest (regression) + IsolationForest + th·ªëng k√™ P10/P90.
    """)
if page == "Business Problem":
    render_business_problem()

# Prediction page
if page == "Prediction":
    st.title(" D·ª± ƒëo√°n gi√° & Ki·ªÉm tra b·∫•t th∆∞·ªùng ‚Äî Xe m√°y c≈©")
    st.markdown("Ch·ªçn: Nh·∫≠p tay ho·∫∑c Upload file CSV/XLSX (c·∫ßn c·ªôt Xu·∫•t_x·ª© cho qu·ªëc gia).")

    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô", ["Nh·∫≠p tay", "Upload file (CSV/XLSX)"], index=0)

    if mode == "Nh·∫≠p tay":
        st.subheader("Nh·∫≠p chi ti·∫øt tin ƒëƒÉng")
        with st.form("form_single", clear_on_submit=False):
            col1, col2 = st.columns([2,1])
            with col1:
                title = st.text_input("Ti√™u ƒë·ªÅ tin ƒëƒÉng", value="B√°n SH Mode 125 ch√≠nh ch·ªß")
                description = st.text_area("M√¥ t·∫£ chi ti·∫øt", value="Xe ƒë·∫πp, bao test, bi·ªÉn s·ªë TP, gi√° c√≥ th∆∞∆°ng l∆∞·ª£ng.")
                address = st.text_input("ƒê·ªãa ch·ªâ", value="Qu·∫≠n 1, TP. H·ªì Ch√≠ Minh")
                brands = sample_df['Th∆∞∆°ng hi·ªáu'].dropna().unique().tolist() if 'Th∆∞∆°ng hi·ªáu' in sample_df.columns else []
                brands = sorted(brands) if brands else ['unknown']
                brand = st.selectbox("Th∆∞∆°ng hi·ªáu", options=brands)
                model_name = st.text_input("D√≤ng xe", value="")
                loai_values = sample_df['Lo·∫°i xe'].dropna().unique().tolist() if 'Lo·∫°i xe' in sample_df.columns else []
                loai = st.selectbox("Lo·∫°i xe", options=sorted(loai_values) if loai_values else ['unknown'])
            with col2:
                dungtich = st.text_input("Dung t√≠ch xe (v√≠ d·ª• '100 - 175 cc' ho·∫∑c '125')", value="125")
                xuatxu = st.text_input("Xu·∫•t x·ª© (Qu·ªëc gia)", value="Vi·ªát Nam")
                age = st.slider("Tu·ªïi xe (nƒÉm)", 0, 50, 3)
                year_reg = int(CURRENT_YEAR - age)
                st.write(f"NƒÉm ƒëƒÉng k√Ω (t∆∞∆°ng ·ª©ng): {year_reg}")
                km = st.number_input("S·ªë Km ƒë√£ ƒëi", min_value=0, max_value=500000, value=20000, step=1000)
                price_input = st.number_input("Gi√° th·ª±c (Tri·ªáu VNƒê) ‚Äî n·∫øu mu·ªën (t√πy ch·ªçn)", min_value=0.0, value=0.0, step=0.1, format="%.2f")
                price_min = st.number_input("Kho·∫£ng_gi√°_min (Tri·ªáu) ‚Äî c√≥ th·ªÉ b·ªè tr·ªëng", min_value=0.0, value=0.0, step=0.1, format="%.2f")
                price_max = st.number_input("Kho·∫£ng_gi√°_max (Tri·ªáu) ‚Äî c√≥ th·ªÉ b·ªè tr·ªëng", min_value=0.0, value=0.0, step=0.1, format="%.2f")
            publish = st.checkbox("L∆∞u ƒë·ªÉ Admin duy·ªát (ƒëƒÉng b√°n)")
            submitted = st.form_submit_button("Predict & Check Anomaly")

        if submitted:
            input_df = pd.DataFrame([{
                "Th∆∞∆°ng hi·ªáu": brand,
                "D√≤ng xe": model_name if model_name.strip()!="" else "unknown",
                "NƒÉm ƒëƒÉng k√Ω": year_reg,
                "S·ªë Km ƒë√£ ƒëi": km,
                "T√¨nh tr·∫°ng": "ƒê√£ s·ª≠ d·ª•ng",
                "Lo·∫°i xe": loai,
                "Dung t√≠ch xe": dungtich,
                "Xu·∫•t x·ª©": xuatxu
            }])
            input_df["NƒÉm ƒëƒÉng k√Ω"] = pd.to_numeric(input_df["NƒÉm ƒëƒÉng k√Ω"], errors="coerce")
            input_df["S·ªë Km ƒë√£ ƒëi"] = pd.to_numeric(input_df["S·ªë Km ƒë√£ ƒëi"], errors="coerce")

            try:
                pred = model.predict(input_df)[0]
            except Exception as e:
                st.error("L·ªói khi d·ª± ƒëo√°n. Ki·ªÉm tra model pipeline.")
                st.write(str(e))
                st.stop()

            # find preprocessor
            pre = model.named_steps.get('pre', model.named_steps.get('preproc', None))
            if pre is None:
                for name, step in model.named_steps.items():
                    if hasattr(step, "transform"):
                        pre = step
                        break
            if pre is None:
                st.error("Kh√¥ng t√¨m th·∫•y preprocessor trong pipeline. Ki·ªÉm tra rf_pipeline.pkl.")
                st.stop()

            X_trans = pre.transform(input_df)
            if hasattr(X_trans, "toarray"):
                X_trans = X_trans.toarray()
            X_trans = np.asarray(X_trans)
            resid_val = (price_input - pred) if price_input > 0 else (0.0 - pred)
            iso_vec = np.hstack([X_trans, np.array(resid_val).reshape(1,1)])
            # ensure iso vec dims
            try:
                expected = iso.n_features_in_
                if iso_vec.shape[1] != expected:
                    st.warning(f"ISO expects {expected} features but got {iso_vec.shape[1]}. Padding/truncating.")
                    if iso_vec.shape[1] < expected:
                        iso_vec = np.hstack([iso_vec, np.zeros((1, expected - iso_vec.shape[1]))])
                    else:
                        iso_vec = iso_vec[:, :expected]
            except Exception:
                pass

            final_score, details = compute_anomaly_score_v2(sample_df=sample_df, brand=brand,
                                                            actual_price=(price_input if price_input>0 else np.nan),
                                                            pred=pred, iso=iso, X_trans_for_iso=iso_vec)

            verdict = "B√¨nh th∆∞·ªùng"
            if final_score >= 50 and details["gap_pct"] is not None and (float(price_input) < float(pred)):
                verdict = "Gi√° th·∫•p b·∫•t th∆∞·ªùng"
            elif final_score >= 50 and details["gap_pct"] is not None and (float(price_input) > float(pred)):
                verdict = "Gi√° cao b·∫•t th∆∞·ªùng"

            # display user-friendly summary
            st.header("K·∫æT QU·∫¢ T√ìM T·∫ÆT")
            st.write(f"- **Gi√° d·ª± ƒëo√°n:** {human_currency_trieu(pred)}")
            if details["has_price"]:
                st.write(f"- **Gi√° th·ª±c b·∫°n nh·∫≠p:** {human_currency_trieu(price_input)}")
                st.write(f"- **Price gap:** {details['gap_pct']:.1f}%")
            else:
                st.write("- B·∫°n **kh√¥ng nh·∫≠p** gi√° th·ª±c ‚Äî ch·ªâ hi·ªán gi√° d·ª± ƒëo√°n v√† ƒë√°nh gi√° r·ªßi ro th·ªã tr∆∞·ªùng.")
            st.metric("Price Risk Score (0=low ‚Üí 100=high)", f"{details['final_score']:.1f}")
            if verdict != "B√¨nh th∆∞·ªùng":
                st.error(f"üî¥ K·∫øt lu·∫≠n: {verdict}")
            else:
                st.success("‚úÖ K·∫øt lu·∫≠n: B√¨nh th∆∞·ªùng")

            st.markdown("**Gi·∫£i th√≠ch chi ti·∫øt:**")
            for s in details["explanations"]:
                st.write(s)

            # detail table
            detail_table = pd.DataFrame([{
                "Gi√°_d·ª±_ƒëo√°n (Tri·ªáu)": pred,
                "Gi√°_th·ª±c (Tri·ªáu n·∫øu c√≥)": (price_input if price_input>0 else np.nan),
                "Gap_pct": details["gap_pct"],
                "Score_gap": details["score_gap"],
                "Score_P10P90": details["score_p10p90"],
                "Score_ISO": details["score_iso"],
                "ISO_flag": details["iso_flag"],
                "ISO_score_raw": details["iso_score_raw"],
                "FinalScore": details["final_score"]
            }])
            st.dataframe(detail_table.T, width=900)

            if publish:
                entry = {
                    "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                    "Ti√™u_ƒë·ªÅ": title,
                    "M√¥_t·∫£_chi_ti·∫øt": description,
                    "ƒê·ªãa_ch·ªâ": address,
                    "Th∆∞∆°ng hi·ªáu": brand,
                    "D√≤ng xe": model_name,
                    "NƒÉm ƒëƒÉng k√Ω": year_reg,
                    "S·ªë Km ƒë√£ ƒëi": km,
                    "Lo·∫°i xe": loai,
                    "Dung t√≠ch xe": dungtich,
                    "Xu·∫•t x·ª©": xuatxu,
                    "Gi√°_th·ª±c": (price_input if price_input>0 else np.nan),
                    "Gi√°_d·ª±_ƒëo√°n": float(pred),
                    "anomaly_score": float(details["final_score"]),
                    "iso_flag": int(details["iso_flag"]),
                    "status": "pending",
                    "notes": ""
                }
                pid = add_pending(entry)
                st.success(f"ƒê√£ l∆∞u id={pid} ch·ªù Admin duy·ªát.")

            log_prediction({
                "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                "mode": "single",
                "title": title,
                "pred": float(pred),
                "price_input": (price_input if price_input>0 else np.nan),
                "anomaly_score": float(details["final_score"]),
                "verdict": verdict
            })

    else:
        # Batch upload
        st.subheader("Upload file CSV/XLSX (batch)")
        st.markdown("File c·∫ßn c√≥ c·ªôt: Th∆∞∆°ng_hi·ªáu, D√≤ng_xe, Lo·∫°i_xe, Dung_t√≠ch_xe, NƒÉm_ƒëƒÉng_k√Ω, S·ªë_Km_ƒë√£_ƒëi, Gi√° (t√πy ch·ªçn), Kho·∫£ng_gi√°_min, Kho·∫£ng_gi√°_max, Ti√™u_ƒë·ªÅ, M√¥_t·∫£_chi_ti·∫øt, ƒê·ªãa_ch·ªâ, Xu·∫•t_x·ª©")
        uploaded = st.file_uploader("Ch·ªçn file", type=["csv","xlsx"])
        if uploaded is not None:
            try:
                if str(uploaded.name).lower().endswith(".csv"):
                    df_up = pd.read_csv(uploaded)
                else:
                    df_up = pd.read_excel(uploaded)
            except Exception as e:
                st.error("Kh√¥ng th·ªÉ ƒë·ªçc file. Ki·ªÉm tra ƒë·ªãnh d·∫°ng.")
                st.write(e)
                df_up = None

            if df_up is not None:
                missing = ensure_cols_for_upload(df_up)
                if missing:
                    st.error(f"File thi·∫øu c·ªôt b·∫Øt bu·ªôc: {missing}")
                else:
                    rename_map = {
                        "Th∆∞∆°ng_hi·ªáu": "Th∆∞∆°ng hi·ªáu",
                        "D√≤ng_xe": "D√≤ng xe",
                        "Lo·∫°i_xe": "Lo·∫°i xe",
                        "Dung_t√≠ch_xe": "Dung t√≠ch xe",
                        "NƒÉm_ƒëƒÉng_k√Ω": "NƒÉm ƒëƒÉng k√Ω",
                        "S·ªë_Km_ƒë√£_ƒëi": "S·ªë Km ƒë√£ ƒëi",
                        "Gi√°": "Gi√°_th·ª±c",
                        "Kho·∫£ng_gi√°_min": "Kho·∫£ng gi√° min",
                        "Kho·∫£ng_gi√°_max": "Kho·∫£ng gi√° max",
                        "Ti√™u_ƒë·ªÅ": "Ti√™u_ƒë·ªÅ",
                        "M√¥_t·∫£_chi_ti·∫øt": "M√¥_t·∫£_chi_ti·∫øt",
                        "ƒê·ªãa_ch·ªâ": "ƒê·ªãa_ch·ªâ",
                        "Xu·∫•t_x·ª©": "Xu·∫•t x·ª©"
                    }
                    df_up = df_up.rename(columns=rename_map)
                    model_inputs = []
                    for _, row in df_up.iterrows():
                        input_row = {
                            "Th∆∞∆°ng hi·ªáu": row["Th∆∞∆°ng hi·ªáu"],
                            "D√≤ng xe": row["D√≤ng xe"] if pd.notna(row["D√≤ng xe"]) else "unknown",
                            "NƒÉm ƒëƒÉng k√Ω": int(row["NƒÉm ƒëƒÉng k√Ω"]) if pd.notna(row["NƒÉm ƒëƒÉng k√Ω"]) else CURRENT_YEAR,
                            "S·ªë Km ƒë√£ ƒëi": int(row["S·ªë Km ƒë√£ ƒëi"]) if pd.notna(row["S·ªë Km ƒë√£ ƒëi"]) else 0,
                            "T√¨nh tr·∫°ng": row.get("T√¨nh tr·∫°ng", "ƒê√£ s·ª≠ d·ª•ng"),
                            "Lo·∫°i xe": row["Lo·∫°i xe"],
                            "Dung t√≠ch xe": row["Dung t√≠ch xe"],
                            "Xu·∫•t x·ª©": row.get("Xu·∫•t x·ª©", "unknown")
                        }
                        model_inputs.append(input_row)
                    model_X = pd.DataFrame(model_inputs)
                    preds = model.predict(model_X)
                    pre = model.named_steps.get('pre', model.named_steps.get('preproc', None))
                    if pre is None:
                        for name, step in model.named_steps.items():
                            if hasattr(step, "transform"):
                                pre = step
                                break
                    if pre is None:
                        st.error("Kh√¥ng t√¨m th·∫•y preprocessor trong pipeline.")
                        st.stop()
                    X_trans = pre.transform(model_X)
                    if hasattr(X_trans, "toarray"):
                        X_trans = X_trans.toarray()
                    X_trans = np.asarray(X_trans)

                    results = []
                    for i in range(len(model_X)):
                        actual_price = df_up.loc[i, "Gi√°_th·ª±c"] if "Gi√°_th·ª±c" in df_up.columns else np.nan
                        pred_i = float(preds[i])
                        resid_val = (actual_price - pred_i) if (pd.notna(actual_price) and actual_price>0) else (0.0 - pred_i)
                        iso_vec = np.hstack([X_trans[i].reshape(1,-1), np.array(resid_val).reshape(1,1)])
                        try:
                            expected = iso.n_features_in_
                            if iso_vec.shape[1] != expected:
                                if iso_vec.shape[1] < expected:
                                    iso_vec = np.hstack([iso_vec, np.zeros((1, expected - iso_vec.shape[1]))])
                                else:
                                    iso_vec = iso_vec[:, :expected]
                        except Exception:
                            pass
                        final_score, details = compute_anomaly_score_v2(sample_df=sample_df,
                                                                     brand=model_X.loc[i, "Th∆∞∆°ng hi·ªáu"],
                                                                     actual_price=(actual_price if pd.notna(actual_price) and actual_price>0 else np.nan),
                                                                     pred=pred_i, iso=iso, X_trans_for_iso=iso_vec)
                        verdict = "B√¨nh th∆∞·ªùng"
                        if final_score >= 50 and (details["gap_pct"] is not None) and (float(actual_price) < pred_i):
                            verdict = "Gi√° th·∫•p b·∫•t th∆∞·ªùng"
                        elif final_score >= 50 and (details["gap_pct"] is not None) and (float(actual_price) > pred_i):
                            verdict = "Gi√° cao b·∫•t th∆∞·ªùng"
                        results.append({
                            "Ti√™u_ƒë·ªÅ": df_up.loc[i, "Ti√™u_ƒë·ªÅ"] if "Ti√™u_ƒë·ªÅ" in df_up.columns else "",
                            "Th∆∞∆°ng hi·ªáu": model_X.loc[i, "Th∆∞∆°ng hi·ªáu"],
                            "D√≤ng xe": model_X.loc[i, "D√≤ng xe"],
                            "Xu·∫•t x·ª©": model_X.loc[i, "Xu·∫•t x·ª©"],
                            "Gi√°_th·ª±c": actual_price if pd.notna(actual_price) else np.nan,
                            "Gi√°_d·ª±_ƒëo√°n": pred_i,
                            "Resid": details["gap_pct"],
                            "ISO_flag": details["iso_flag"],
                            "AnomalyScore": final_score,
                            "Verdict": verdict
                        })
                        log_prediction({
                            "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                            "mode": "batch",
                            "file": uploaded.name,
                            "pred": float(pred_i),
                            "price_input": float(actual_price) if pd.notna(actual_price) else np.nan,
                            "anomaly_score": float(final_score),
                            "verdict": verdict
                        })
                    res_df = pd.DataFrame(results)
                    st.success("X·ª≠ l√Ω xong ‚Äî hi·ªÉn th·ªã k·∫øt qu·∫£")
                    st.dataframe(res_df)
                    st.download_button("Export k·∫øt qu·∫£ (CSV)", data=res_df.to_csv(index=False).encode('utf-8'), file_name="batch_predictions.csv", mime="text/csv")

# Anomaly check quick page
if page == "Anomaly Check":
    st.title("üîé Ki·ªÉm tra b·∫•t th∆∞·ªùng (nhanh)")
    with st.form("anom_quick"):
        brand = st.text_input("Th∆∞∆°ng hi·ªáu", value="unknown")
        model_name = st.text_input("D√≤ng xe", value="unknown")
        age = st.slider("Tu·ªïi xe (nƒÉm)", min_value=0, max_value=50, value=3)
        year_registered = int(CURRENT_YEAR - age)
        km = st.number_input("S·ªë Km ƒë√£ ƒëi", min_value=0, max_value=500000, value=20000, step=1000)
        loai = st.text_input("Lo·∫°i xe", value="unknown")
        dungtich = st.text_input("Dung t√≠ch xe", value="125")
        xuatxu = st.text_input("Xu·∫•t x·ª©", value="unknown")
        gia_thuc = st.number_input("Gi√° th·ª±c (Tri·ªáu VNƒê)", min_value=0.0, value=0.0, step=0.1, format="%.2f")
        submitted = st.form_submit_button("Check Anomaly")
    if submitted:
        input_df = pd.DataFrame([{
            "Th∆∞∆°ng hi·ªáu": brand,
            "D√≤ng xe": model_name,
            "NƒÉm ƒëƒÉng k√Ω": year_registered,
            "S·ªë Km ƒë√£ ƒëi": km,
            "Lo·∫°i xe": loai,
            "Dung t√≠ch xe": dungtich,
            "Xu·∫•t x·ª©": xuatxu
        }])
        input_df["NƒÉm ƒëƒÉng k√Ω"] = pd.to_numeric(input_df["NƒÉm ƒëƒÉng k√Ω"], errors="coerce")
        input_df["S·ªë Km ƒë√£ ƒëi"] = pd.to_numeric(input_df["S·ªë Km ƒë√£ ƒëi"], errors="coerce")
        pred = model.predict(input_df)[0]
        pre = model.named_steps.get('pre', model.named_steps.get('preproc', None))
        if pre is None:
            for name, step in model.named_steps.items():
                if hasattr(step, "transform"):
                    pre = step
                    break
        X_trans = pre.transform(input_df)
        if hasattr(X_trans, "toarray"):
            X_trans = X_trans.toarray()
        X_trans = np.asarray(X_trans)
        resid = (gia_thuc - pred) if gia_thuc>0 else (0.0 - pred)
        iso_vec = np.hstack([X_trans, np.array(resid).reshape(1,1)])
        try:
            expected = iso.n_features_in_
            if iso_vec.shape[1] != expected:
                if iso_vec.shape[1] < expected:
                    iso_vec = np.hstack([iso_vec, np.zeros((1, expected - iso_vec.shape[1]))])
                else:
                    iso_vec = iso_vec[:, :expected]
        except Exception:
            pass
        final_score, details = compute_anomaly_score_v2(sample_df=sample_df, brand=brand,
                                                        actual_price=(gia_thuc if gia_thuc>0 else np.nan),
                                                        pred=pred, iso=iso, X_trans_for_iso=iso_vec)
        st.metric("Gi√° d·ª± ƒëo√°n (Tri·ªáu)", f"{pred:.2f}")
        st.metric("Price Risk Score (0-100)", f"{final_score:.1f}")
        if final_score >= 50 and details["gap_pct"] is not None and details["gap_pct"]>0 and (gia_thuc < pred):
            st.error("K·∫øt lu·∫≠n: Gi√° th·∫•p b·∫•t th∆∞·ªùng")
        elif final_score >= 50 and details["gap_pct"] is not None and details["gap_pct"]>0 and (gia_thuc > pred):
            st.error("K·∫øt lu·∫≠n: Gi√° cao b·∫•t th∆∞·ªùng")
        else:
            st.success("K·∫øt lu·∫≠n: B√¨nh th∆∞·ªùng")
        st.write("Gi·∫£i th√≠ch:")
        for line in details["explanations"]:
            st.write(line)

# Admin / Logs / Eval pages (unchanged structure)
if page == "Admin Dashboard":
    st.title("üõ†Ô∏è Admin Dashboard")
    if PENDING_PATH.exists():
        pending = pd.read_csv(PENDING_PATH)
    else:
        pending = pd.DataFrame(columns=["id","timestamp","Th∆∞∆°ng hi·ªáu","D√≤ng xe","Gi√°_th·ª±c","Gi√°_d·ª±_ƒëo√°n","anomaly_score","iso_flag","status","notes"])
    st.write(f"T·ªïng submissions: {len(pending)}")
    st.dataframe(pending.sort_values("timestamp", ascending=False).head(200))
    if len(pending)>0:
        pick = st.selectbox("Ch·ªçn id ƒë·ªÉ thao t√°c", options=["(ch·ªçn)"] + pending["id"].astype(str).tolist())
        if pick != "(ch·ªçn)":
            row = pending[pending["id"].astype(str)==pick].iloc[0]
            st.write(row.to_dict())
            if st.button("Approve"):
                pending.loc[pending["id"]==int(pick),"status"] = "approved"
                pending.to_csv(PENDING_PATH, index=False)
                st.success("ƒê√£ approve")
            if st.button("Reject"):
                pending.loc[pending["id"]==int(pick),"status"] = "rejected"
                pending.to_csv(PENDING_PATH, index=False)
                st.warning("ƒê√£ reject")
            if st.button("Delete"):
                pending = pending[pending["id"]!=int(pick)]
                pending.to_csv(PENDING_PATH, index=False)
                st.info("ƒê√£ x√≥a")
    st.markdown("---")
    try:
        n_trees = model.named_steps['rf'].n_estimators
    except:
        n_trees = "unknown"
    st.write(f"- RandomForest trees: {n_trees}")
    st.write(f"- Training sample size (app sample): {len(sample_df)}")
    st.write("- Anomaly detector: IsolationForest trained on features + residual")
    if FI_CSV.exists():
        st.dataframe(pd.read_csv(FI_CSV).head(30))
    else:
        st.info("feature_importances.csv not found in repo.")

if page == "Logs":
    st.title(" Logs ho·∫°t ƒë·ªông")
    if LOG_PATH.exists():
        logs = pd.read_csv(LOG_PATH)
        st.write(f"T·ªïng b·∫£n ghi: {len(logs)}")
        st.dataframe(logs.sort_values("timestamp", ascending=False).head(500))
        st.download_button("Export Logs CSV", data=logs.to_csv(index=False).encode('utf-8'), file_name="prediction_logs.csv", mime="text/csv")
    else:
        st.info("Ch∆∞a c√≥ logs n√†o")

if page == "Evaluation & Report":
    st.title(" Evaluation & Report")
    st.subheader("Sample data preview")
    st.dataframe(sample_df.head(200))
    st.subheader("Feature importances")
    try:
        if FI_CSV.exists():
            fi = pd.read_csv(FI_CSV)
            st.dataframe(fi.head(50))
            fig, ax = plt.subplots(figsize=(8,4))
            ax.barh(fi['feature'].head(20)[::-1], fi['importance'].head(20)[::-1])
            st.pyplot(fig)
        else:
            st.info("Kh√¥ng t√¨m th·∫•y file feature_importances.csv")
    except Exception as e:
        st.write("Kh√¥ng th·ªÉ hi·ªÉn th·ªã feature importances:", e)

if page == "Team Info":
    st.title("Nh√≥m th·ª±c hi·ªán")
    st.markdown("- H·ªç t√™n HV: Nguyen Thai Binh")
    st.markdown("- Email: thaibinh782k1@gmail.com")
    st.markdown("- Repo: https://github.com/ThaiBinh78/ML07_Project")
    st.markdown("- Ng√†y report: 22/11/2025")
