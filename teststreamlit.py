# app_motor_price.py
import streamlit as st
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import io

# ----------------------
# CONFIG
# ----------------------
BASE_DIR = Path(__file__).resolve().parent
# If you want to use the uploaded models in /mnt/data, use those paths:
DEFAULT_RF_PATH = BASE_DIR / "rf_pipeline.pkl"
DEFAULT_ISO_PATH = BASE_DIR / "isolation_forest.pkl"
DEFAULT_SAMPLE = BASE_DIR / "sample_data.csv"


PENDING_PATH = BASE_DIR / "pending_listings.csv"

MODEL_PATH = DEFAULT_RF_PATH
ISO_PATH = DEFAULT_ISO_PATH
SAMPLE_PATH = DEFAULT_SAMPLE

PENDING_PATH = BASE_DIR / "pending_listings.csv"
LOG_PATH = BASE_DIR / "prediction_logs.csv"

CURRENT_YEAR = datetime.now().year

st.set_page_config(page_title="D·ª± ƒëo√°n gi√° - Xe m√°y c≈©", layout="wide")

# ----------------------
# Helpers
# ----------------------
@st.cache_resource
def load_models_and_sample(rf_path, iso_path, sample_path):
    model = joblib.load(rf_path)
    iso = joblib.load(iso_path)
    sample = pd.read_csv(sample_path)
    # sanitize sample numeric columns
    for col in ["Gia_trieu", "Gi√°", "Kho·∫£ng gi√° min", "Kho·∫£ng gi√° max"]:
        if col in sample.columns:
            sample[col] = pd.to_numeric(sample[col], errors="coerce")
    return model, iso, sample

def ensure_cols_for_upload(df):
    required = [
        "Th∆∞∆°ng_hi·ªáu","D√≤ng_xe","Lo·∫°i_xe","Dung_t√≠ch_xe",
        "NƒÉm_ƒëƒÉng_k√Ω","S·ªë_Km_ƒë√£_ƒëi","Gi√°","Kho·∫£ng_gi√°_min","Kho·∫£ng_gi√°_max",
        "Ti√™u_ƒë·ªÅ","M√¥_t·∫£_chi_ti·∫øt","ƒê·ªãa_ch·ªâ"
    ]
    missing = [c for c in required if c not in df.columns]
    return missing

def add_pending(entry: dict):
    if PENDING_PATH.exists():
        df = pd.read_csv(PENDING_PATH)
    else:
        df = pd.DataFrame()
    entry_id = int(datetime.utcnow().timestamp() * 1000)
    entry["id"] = entry_id
    df = pd.concat([pd.DataFrame([entry]), df], ignore_index=True, sort=False)
    df.to_csv(PENDING_PATH, index=False)
    return entry_id

def log_prediction(record: dict):
    if Path(LOG_PATH).exists():
        logs = pd.read_csv(LOG_PATH)
    else:
        logs = pd.DataFrame()
    logs = pd.concat([pd.DataFrame([record]), logs], ignore_index=True, sort=False)
    logs.to_csv(LOG_PATH, index=False)

def human_currency(x):
    try:
        return f"{int(round(float(x))):,} Tri·ªáu"
    except:
        return x

def compute_anomaly_score(sample_df, brand, actual_price, pred, iso, X_trans_for_iso):
    """
    Compute 4 components:
    1) residual z (brand IQR or global std fallback)
    2) min/max violation
    3) outside [P10,P90]
    4) isolation forest raw score -> normalized
    Return final_score (0-100) and dict of details.
    """
    resid = (actual_price - pred) if not pd.isna(actual_price) else (0 - pred)
    sample_brand = sample_df[sample_df['Th∆∞∆°ng hi·ªáu'] == brand] if 'Th∆∞∆°ng hi·ªáu' in sample_df.columns else pd.DataFrame()
    # resid_z
    if len(sample_brand) >= 10 and 'Gia_trieu' in sample_brand.columns:
        iqr = (sample_brand['Gia_trieu'].quantile(0.75) - sample_brand['Gia_trieu'].quantile(0.25)) or 1.0
        resid_z = abs(resid) / iqr
    else:
        resid_z = abs(resid) / max(1.0, sample_df['Gia_trieu'].std() if 'Gia_trieu' in sample_df.columns else 1.0)
    # min/max
    min_price = sample_brand['Kho·∫£ng gi√° min'].min() if ('Kho·∫£ng gi√° min' in sample_brand.columns and len(sample_brand)>0) else np.nan
    max_price = sample_brand['Kho·∫£ng gi√° max'].max() if ('Kho·∫£ng gi√° max' in sample_brand.columns and len(sample_brand)>0) else np.nan
    violate_minmax = int((not pd.isna(min_price) and actual_price < min_price) or (not pd.isna(max_price) and actual_price > max_price))
    # p10/p90
    p10 = sample_brand['Gia_trieu'].quantile(0.10) if (len(sample_brand)>0 and 'Gia_trieu' in sample_brand.columns) else np.nan
    p90 = sample_brand['Gia_trieu'].quantile(0.90) if (len(sample_brand)>0 and 'Gia_trieu' in sample_brand.columns) else np.nan
    outside_p10p90 = int((not pd.isna(p10) and actual_price < p10) or (not pd.isna(p90) and actual_price > p90))
    # isolation: X_trans_for_iso must include residual appended (1xN)
    iso_vec = X_trans_for_iso
    # ensure dense
    if hasattr(iso_vec, "toarray"):
        iso_vec = iso_vec.toarray()
    iso_vec = np.asarray(iso_vec)
    # predict iso raw score
    try:
        iso_score_raw = - iso.decision_function(iso_vec.reshape(1, -1))[0]
        iso_flag = int(iso.predict(iso_vec.reshape(1, -1))[0] == -1)
    except Exception:
        # fallback to 0
        iso_score_raw = 0.0
        iso_flag = 0
    # combine weights
    w1, w2, w3, w4 = 0.4, 0.2, 0.2, 0.2
    score1 = min(1.0, resid_z / 3.0) * 100.0
    score2 = violate_minmax * 100.0
    score3 = outside_p10p90 * 100.0
    score4 = min(1.0, iso_score_raw / 0.5) * 100.0
    final_score = w1*score1 + w2*score2 + w3*score3 + w4*score4
    return final_score, {
        "resid": float(resid),
        "resid_z": float(resid_z),
        "violate_minmax": int(violate_minmax),
        "outside_p10p90": int(outside_p10p90),
        "iso_flag": int(iso_flag),
        "iso_score_raw": float(iso_score_raw),
        "score_components": {"score1": score1, "score2": score2, "score3": score3, "score4": score4}
    }

# ----------------------
# Load models & sample
# ----------------------
try:
    model, iso, sample_df = load_models_and_sample(MODEL_PATH, ISO_PATH, SAMPLE_PATH)
except Exception as e:
    st.error("Kh√¥ng th·ªÉ load model/sample. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n:")
    st.write(str(e))
    st.stop()

# ----------------------
# Sidebar menu (single page app with sidebar)
# ----------------------
st.sidebar.title("Menu")
st.image("xe_may_cu.jpg")
page = st.sidebar.radio("Ch·ªçn m·ª•c", ["Prediction", "Admin Dashboard", "Logs", "Evaluation & Report", "Team Info"])

# ----------------------
# PREDICTION PAGE (single tab with two options)
# ----------------------
if page == "Prediction":
    st.title(" D·ª± ƒëo√°n gi√° & Ki·ªÉm tra b·∫•t th∆∞·ªùng ‚Äî Xe m√°y c≈©")
    st.markdown("Ch·ªçn c√°ch nh·∫≠p: Nh·∫≠p tay ho·∫∑c Upload file CSV/XLSX (12 c·ªôt chu·∫©n).")

    mode = st.radio("Ch·ªçn ch·∫ø ƒë·ªô", ["Nh·∫≠p tay", "Upload file (CSV/XLSX)"], index=0)

    if mode == "Nh·∫≠p tay":
        st.subheader("Nh·∫≠p chi ti·∫øt tin ƒëƒÉng")
        with st.form("form_single", clear_on_submit=False):
            col1, col2 = st.columns([2,1])
            with col1:
                title = st.text_input("Ti√™u ƒë·ªÅ tin ƒëƒÉng", value="B√°n SH Mode 125 ch√≠nh ch·ªß")
                description = st.text_area("M√¥ t·∫£ chi ti·∫øt", value="Xe ƒë·∫πp, bao test, bi·ªÉn s·ªë TP, gi√° c√≥ th∆∞∆°ng l∆∞·ª£ng.")
                address = st.text_input("ƒê·ªãa ch·ªâ", value="Qu·∫≠n 1, TP. H·ªì Ch√≠ Minh")
                brand = st.selectbox("Th∆∞∆°ng hi·ªáu", options=sorted(sample_df['Th∆∞∆°ng hi·ªáu'].dropna().unique().tolist()))
                model_name = st.text_input("D√≤ng xe", value="")
                loai = st.selectbox("Lo·∫°i xe", options=sorted(sample_df['Lo·∫°i xe'].dropna().unique().tolist()))
            with col2:
                dungtich = st.text_input("Dung t√≠ch xe (v√≠ d·ª• '100 - 175 cc' ho·∫∑c '125')", value="125")
                age = st.slider("Tu·ªïi xe (nƒÉm)", 0, 50, 3)
                year_reg = int(CURRENT_YEAR - age)
                st.write(f"NƒÉm ƒëƒÉng k√Ω (t∆∞∆°ng ·ª©ng): {year_reg}")
                km = st.number_input("S·ªë Km ƒë√£ ƒëi", min_value=0, max_value=500000, value=20000, step=1000)
                price_input = st.number_input("Gi√° th·ª±c (VNƒê) ‚Äî n·∫øu mu·ªën (t√πy ch·ªçn)", min_value=0, value=0, step=100000)
                price_min = st.number_input("Kho·∫£ng_gi√°_min (VNƒê) ‚Äî c√≥ th·ªÉ b·ªè tr·ªëng", min_value=0, value=0, step=100000)
                price_max = st.number_input("Kho·∫£ng_gi√°_max (VNƒê) ‚Äî c√≥ th·ªÉ b·ªè tr·ªëng", min_value=0, value=0, step=100000)

            publish = st.checkbox("L∆∞u ƒë·ªÉ Admin duy·ªát (ƒëƒÉng b√°n)")
            submitted = st.form_submit_button("Predict & Check Anomaly")

        if submitted:
            # Build input df with correct training column names:
            input_df = pd.DataFrame([{
                "Th∆∞∆°ng hi·ªáu": brand,
                "D√≤ng xe": model_name if model_name.strip()!="" else "unknown",
                "NƒÉm ƒëƒÉng k√Ω": year_reg,
                "S·ªë Km ƒë√£ ƒëi": km,
                "T√¨nh tr·∫°ng": "ƒê√£ s·ª≠ d·ª•ng",
                "Lo·∫°i xe": loai,
                "Dung t√≠ch xe": dungtich,
                "Xu·∫•t x·ª©": "unknown"
            }])

            # sanitize types
            input_df["NƒÉm ƒëƒÉng k√Ω"] = pd.to_numeric(input_df["NƒÉm ƒëƒÉng k√Ω"], errors="coerce")
            input_df["S·ªë Km ƒë√£ ƒëi"] = pd.to_numeric(input_df["S·ªë Km ƒë√£ ƒëi"], errors="coerce")

            # predict
            pred = model.predict(input_df)[0]
            # transform features for ISO (and append residual)
            pre = model.named_steps['pre']
            X_trans = pre.transform(input_df)
            if hasattr(X_trans, "toarray"):
                X_trans = X_trans.toarray()
            X_trans = np.asarray(X_trans)
            # compute residual for ISO training alignment: ISO expects features + residual (1 column)
            resid_val = (price_input - pred) if price_input > 0 else (0 - pred)
            resid_col = np.array(resid_val).reshape(1,1)
            iso_vec = np.hstack([X_trans, resid_col])
            # compute iso decision
            iso_flag = int(iso.predict(iso_vec)[0] == -1)
            iso_score_raw = float(-iso.decision_function(iso_vec)[0])
            # compute anomaly score using helper
            final_score, details = compute_anomaly_score(sample_df=sample_df, brand=brand,
                                                         actual_price=(price_input if price_input>0 else np.nan),
                                                         pred=pred, iso=iso, X_trans_for_iso=iso_vec.flatten())
            # determine verdict
            verdict = "B√¨nh th∆∞·ªùng"
            if final_score >= 50 and (details["resid"] < 0):
                verdict = "Gi√° th·∫•p b·∫•t th∆∞·ªùng"
            elif final_score >= 50 and (details["resid"] > 0):
                verdict = "Gi√° cao b·∫•t th∆∞·ªùng"

            # display
            st.markdown("### K·∫øt qu·∫£ d·ª± ƒëo√°n")
            st.write(f"**Gi√° d·ª± ƒëo√°n:** {human_currency(pred)}")
            st.metric("Anomaly Score (0-100)", f"{final_score:.1f}")
            if verdict != "B√¨nh th∆∞·ªùng":
                st.error(f" K·∫øt lu·∫≠n: {verdict}")
            else:
                st.success(" K·∫øt lu·∫≠n: B√¨nh th∆∞·ªùng")
            st.markdown("**L√Ω do:**")
            reasons = []
            if details["resid_z"] > 1.5:
                reasons.append("- Residual Z cao (kh√°c bi·ªát l·ªõn so v·ªõi ph√¢n kh√∫c).")
            if details["violate_minmax"]:
                reasons.append("- Vi ph·∫°m kho·∫£ng gi√° min/max c·ªßa th∆∞∆°ng hi·ªáu.")
            if details["outside_p10p90"]:
                reasons.append("- Gi√° n·∫±m ngo√†i P10‚ÄìP90 theo th∆∞∆°ng hi·ªáu.")
            if details["iso_flag"]:
                reasons.append("- IsolationForest ƒë√°nh d·∫•u b·∫•t th∆∞·ªùng d·ª±a tr√™n vector ƒë·∫∑c tr∆∞ng + resid.")
            if not reasons:
                reasons.append("- Kh√¥ng ph√°t hi·ªán ƒëi·ªÉm b·∫•t th∆∞·ªùng r√µ r·ªát.")
            for r in reasons:
                st.write(r)

            # detailed table
            detail_table = pd.DataFrame([{
                "Gi√°_d·ª±_ƒëo√°n": pred,
                "Gi√°_th·ª±c (n·∫øu c√≥)": (price_input if price_input>0 else np.nan),
                "Resid": details["resid"],
                "Resid_z": details["resid_z"],
                "Violate_minmax": details["violate_minmax"],
                "Outside_P10_P90": details["outside_p10p90"],
                "ISO_flag": details["iso_flag"],
                "ISO_score_raw": details["iso_score_raw"],
                "AnomalyScore": final_score
            }])
            st.dataframe(detail_table.T, width=900)

            # save pending if requested
            if publish:
                entry = {
                    "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                    "Ti√™u_ƒë·ªÅ": title,
                    "M√¥_t·∫£_chi_ti·∫øt": description,
                    "ƒê·ªãa_ch·ªâ": address,
                    "Th∆∞∆°ng hi·ªáu": brand,
                    "D√≤ng xe": model_name,
                    "NƒÉm ƒëƒÉng k√Ω": year_reg,
                    "S·ªë Km ƒë√£ ƒëi": km,
                    "Lo·∫°i xe": loai,
                    "Dung t√≠ch xe": dungtich,
                    "Gi√°_th·ª±c": (price_input if price_input>0 else np.nan),
                    "Gi√°_d·ª±_ƒëo√°n": float(pred),
                    "anomaly_score": float(final_score),
                    "iso_flag": int(details["iso_flag"]),
                    "status": "pending",
                    "notes": ""
                }
                pid = add_pending(entry)
                st.success(f"K·∫øt qu·∫£ ƒë√£ l∆∞u (id={pid}) v√† ch·ªù Admin duy·ªát.")

            # log
            log_record = {
                "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                "mode": "single",
                "title": title,
                "pred": float(pred),
                "price_input": (price_input if price_input>0 else np.nan),
                "anomaly_score": float(final_score),
                "verdict": verdict
            }
            log_prediction(log_record)

    else:
        st.subheader("Upload file CSV/XLSX (batch)")
        st.markdown("File c·∫ßn c√≥ c√°c c·ªôt: Th∆∞∆°ng_hi·ªáu, D√≤ng_xe, Lo·∫°i_xe, Dung_t√≠ch_xe, NƒÉm_ƒëƒÉng_k√Ω, S·ªë_Km_ƒë√£_ƒëi, Gi√° (t√πy ch·ªçn), Kho·∫£ng_gi√°_min, Kho·∫£ng_gi√°_max, Ti√™u_ƒë·ªÅ, M√¥_t·∫£_chi_ti·∫øt, ƒê·ªãa_ch·ªâ")
        uploaded = st.file_uploader("Ch·ªçn file", type=["csv","xlsx"])
        if uploaded is not None:
            try:
                if str(uploaded.name).lower().endswith(".csv"):
                    df_up = pd.read_csv(uploaded)
                else:
                    df_up = pd.read_excel(uploaded)
            except Exception as e:
                st.error("Kh√¥ng th·ªÉ ƒë·ªçc file. Ki·ªÉm tra ƒë·ªãnh d·∫°ng.")
                st.write(e)
                df_up = None
            if df_up is not None:
                missing = ensure_cols_for_upload(df_up)
                if missing:
                    st.error(f"File thi·∫øu c·ªôt b·∫Øt bu·ªôc: {missing}")
                else:
                    # rename upload columns -> training schema
                    # map upload names to model names:
                    rename_map = {
                        "Th∆∞∆°ng_hi·ªáu": "Th∆∞∆°ng hi·ªáu",
                        "D√≤ng_xe": "D√≤ng xe",
                        "Lo·∫°i_xe": "Lo·∫°i xe",
                        "Dung_t√≠ch_xe": "Dung t√≠ch xe",
                        "NƒÉm_ƒëƒÉng_k√Ω": "NƒÉm ƒëƒÉng k√Ω",
                        "S·ªë_Km_ƒë√£_ƒëi": "S·ªë Km ƒë√£ ƒëi",
                        "Gi√°": "Gi√°_th·ª±c",
                        "Kho·∫£ng_gi√°_min": "Kho·∫£ng gi√° min",
                        "Kho·∫£ng_gi√°_max": "Kho·∫£ng gi√° max",
                        "Ti√™u_ƒë·ªÅ": "Ti√™u_ƒë·ªÅ",
                        "M√¥_t·∫£_chi_ti·∫øt": "M√¥_t·∫£_chi_ti·∫øt",
                        "ƒê·ªãa_ch·ªâ": "ƒê·ªãa_ch·ªâ"
                    }
                    df_up = df_up.rename(columns=rename_map)
                    # build input for model
                    model_inputs = []
                    out_rows = []
                    for _, row in df_up.iterrows():
                        input_row = {
                            "Th∆∞∆°ng hi·ªáu": row["Th∆∞∆°ng hi·ªáu"],
                            "D√≤ng xe": row["D√≤ng xe"] if pd.notna(row["D√≤ng xe"]) else "unknown",
                            "NƒÉm ƒëƒÉng k√Ω": int(row["NƒÉm ƒëƒÉng k√Ω"]) if pd.notna(row["NƒÉm ƒëƒÉng k√Ω"]) else CURRENT_YEAR,
                            "S·ªë Km ƒë√£ ƒëi": int(row["S·ªë Km ƒë√£ ƒëi"]) if pd.notna(row["S·ªë Km ƒë√£ ƒëi"]) else 0,
                            "T√¨nh tr·∫°ng": row.get("T√¨nh tr·∫°ng", "ƒê√£ s·ª≠ d·ª•ng"),
                            "Lo·∫°i xe": row["Lo·∫°i xe"],
                            "Dung t√≠ch xe": row["Dung t√≠ch xe"],
                            "Xu·∫•t x·ª©": row.get("Xu·∫•t x·ª©", "unknown")
                        }
                        model_inputs.append(input_row)
                    model_X = pd.DataFrame(model_inputs)
                    # predict batch
                    preds = model.predict(model_X)
                    # transform base features
                    pre = model.named_steps['pre']
                    X_trans = pre.transform(model_X)
                    if hasattr(X_trans, "toarray"):
                        X_trans = X_trans.toarray()
                    X_trans = np.asarray(X_trans)
                    # prepare results
                    results = []
                    for i in range(len(model_X)):
                        actual_price = df_up.loc[i, "Gi√°"] if "Gi√°" in df_up.columns else np.nan
                        pred_i = preds[i]
                        resid_val = (actual_price - pred_i) if (pd.notna(actual_price) and actual_price>0) else (0 - pred_i)
                        iso_vec = np.hstack([X_trans[i].reshape(1,-1), np.array(resid_val).reshape(1,1)])
                        final_score, details = compute_anomaly_score(sample_df=sample_df,
                                                                     brand=model_X.loc[i, "Th∆∞∆°ng hi·ªáu"],
                                                                     actual_price=(actual_price if pd.notna(actual_price) and actual_price>0 else np.nan),
                                                                     pred=pred_i, iso=iso, X_trans_for_iso=iso_vec.flatten())
                        verdict = "B√¨nh th∆∞·ªùng"
                        if final_score >= 50 and (details["resid"] < 0):
                            verdict = "Gi√° th·∫•p b·∫•t th∆∞·ªùng"
                        elif final_score >= 50 and (details["resid"] > 0):
                            verdict = "Gi√° cao b·∫•t th∆∞·ªùng"
                        results.append({
                            "Ti√™u_ƒë·ªÅ": df_up.loc[i, "Ti√™u_ƒë·ªÅ"] if "Ti√™u_ƒë·ªÅ" in df_up.columns else "",
                            "Th∆∞∆°ng hi·ªáu": model_X.loc[i, "Th∆∞∆°ng hi·ªáu"],
                            "D√≤ng xe": model_X.loc[i, "D√≤ng xe"],
                            "Gi√°_th·ª±c": actual_price if pd.notna(actual_price) else np.nan,
                            "Gi√°_d·ª±_ƒëo√°n": pred_i,
                            "Resid": details["resid"],
                            "Resid_z": details["resid_z"],
                            "ISO_flag": details["iso_flag"],
                            "ISO_score_raw": details["iso_score_raw"],
                            "AnomalyScore": final_score,
                            "Verdict": verdict
                        })
                        # optionally save pending if needed - we won't auto-save here
                        # log entry
                        log_prediction({
                            "timestamp": datetime.now().isoformat(sep=' ', timespec='seconds'),
                            "mode": "batch",
                            "file": uploaded.name,
                            "pred": float(pred_i),
                            "price_input": float(actual_price) if pd.notna(actual_price) else np.nan,
                            "anomaly_score": float(final_score),
                            "verdict": verdict
                        })
                    res_df = pd.DataFrame(results)
                    st.success("X·ª≠ l√Ω xong ‚Äî hi·ªÉn th·ªã k·∫øt qu·∫£")
                    # display table + export
                    st.dataframe(res_df)
                    csv = res_df.to_csv(index=False).encode('utf-8')
                    st.download_button("Export k·∫øt qu·∫£ (CSV)", data=csv, file_name="batch_predictions.csv", mime="text/csv")

# ----------------------
# ADMIN DASHBOARD
# ----------------------
elif page == "Admin Dashboard":
    st.title("üõ†Ô∏è Admin Dashboard")
    st.markdown("Duy·ªát c√°c submissions t·ª´ ng∆∞·ªùi d√πng")
    # show pending
    if PENDING_PATH.exists():
        pending = pd.read_csv(PENDING_PATH)
    else:
        pending = pd.DataFrame(columns=["id","timestamp","Th∆∞∆°ng hi·ªáu","D√≤ng xe","Gi√°_th·ª±c","Gi√°_d·ª±_ƒëo√°n","anomaly_score","iso_flag","status","notes"])
    st.write(f"T·ªïng submissions: {len(pending)}")
    st.dataframe(pending.sort_values("timestamp", ascending=False).head(200))
    # operate
    if len(pending)>0:
        pick = st.selectbox("Ch·ªçn id ƒë·ªÉ thao t√°c", options=["(ch·ªçn)"] + pending["id"].astype(str).tolist())
        if pick != "(ch·ªçn)":
            row = pending[pending["id"].astype(str)==pick].iloc[0]
            st.write(row.to_dict())
            if st.button("Approve"):
                pending.loc[pending["id"]==int(pick),"status"] = "approved"
                pending.to_csv(PENDING_PATH, index=False)
                st.success("ƒê√£ approve")
            if st.button("Reject"):
                pending.loc[pending["id"]==int(pick),"status"] = "rejected"
                pending.to_csv(PENDING_PATH, index=False)
                st.warning("ƒê√£ reject")
            if st.button("Delete"):
                pending = pending[pending["id"]!=int(pick)]
                pending.to_csv(PENDING_PATH, index=False)
                st.info("ƒê√£ x√≥a")

    st.markdown("---")
    st.subheader("Th√¥ng tin model")
    try:
        n_trees = model.named_steps['rf'].n_estimators
    except:
        n_trees = "unknown"
    st.write(f"- RandomForest trees: {n_trees}")
    st.write(f"- Training sample size (app sample): {len(sample_df)}")
    st.write("- Anomaly detector: IsolationForest trained on features + residual")
    st.dataframe(pd.read_csv(BASE_DIR / "feature_importances.csv").head(30))

# ----------------------
# LOGS PAGE
# ----------------------
elif page == "Logs":
    st.title(" Logs ho·∫°t ƒë·ªông")
    if Path(LOG_PATH).exists():
        logs = pd.read_csv(LOG_PATH)
        st.write(f"T·ªïng b·∫£n ghi: {len(logs)}")
        st.dataframe(logs.sort_values("timestamp", ascending=False).head(500))
        st.download_button("Export Logs CSV", data=logs.to_csv(index=False).encode('utf-8'), file_name="prediction_logs.csv", mime="text/csv")
    else:
        st.info("Ch∆∞a c√≥ logs n√†o")

# ----------------------
# EVALUATION & REPORT
# ----------------------
elif page == "Evaluation & Report":
    st.title(" Evaluation & Report")
    st.subheader("Sample data preview")
    st.dataframe(sample_df.head(200))
    st.subheader("Feature importances")
    try:
        fi = pd.read_csv(BASE_DIR / "feature_importances.csv")
        st.dataframe(fi.head(50))
        fig, ax = plt.subplots(figsize=(8,4))
        ax.barh(fi['feature'].head(20)[::-1], fi['importance'].head(20)[::-1])
        st.pyplot(fig)
    except Exception as e:
        st.write("Kh√¥ng t√¨m th·∫•y file feature_importances.csv", e)

# ----------------------
# TEAM INFO
# ----------------------
else:
    st.title("Nh√≥m th·ª±c hi·ªán")
    st.markdown("- H·ªç t√™n HV: Nguyen Thai Binh")
    st.markdown("- Email: thaibinh782k1@gmail.com")
    st.markdown("- Repo: https://github.com/ThaiBinh78/ML07_Project")
    st.markdown("- Ng√†y report: 22/11/2025")
